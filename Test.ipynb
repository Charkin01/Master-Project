{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223338be-54c3-4ba5-ace0-aee415a88261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'expected_answer', 'predicted_answer', 'error_message', 'is_correct', 'generation_type', 'dataset', 'generated_solution']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa232010b3194176809bf0f2355a6c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7321344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"nvidia/OpenMathInstruct-1\")\n",
    "\n",
    "# Print column names of the train split\n",
    "print(dataset['train'].column_names)\n",
    "\n",
    "# Use the correct column name based on inspection\n",
    "text_column = 'question'  # Update this if you find the correct column name is different\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert the Hugging Face dataset to TensorFlow format\n",
    "def convert_to_tf_dataset(tokenized_dataset, batch_size=8):\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        dict(tokenized_dataset.remove_columns([text_column])),\n",
    "        tokenized_dataset[\"is_correct\"]\n",
    "    )).batch(batch_size)\n",
    "\n",
    "train_dataset = convert_to_tf_dataset(tokenized_datasets[\"train\"])\n",
    "eval_dataset = convert_to_tf_dataset(tokenized_datasets[\"validation\"])\n",
    "\n",
    "# Load the BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Enable GPU usage if available\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"Using GPU\")\n",
    "    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Define training arguments\n",
    "epochs = 3\n",
    "batch_size = 8\n",
    "\n",
    "# Prepare the training loop\n",
    "print(\"Setup complete. Training process is prepared but not started.\")\n",
    "\n",
    "# To start the training, uncomment the next line\n",
    "# history = model.fit(train_dataset, validation_data=eval_dataset, epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702eec5b-992f-40b3-96c5-d97458d26bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
